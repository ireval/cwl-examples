{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the output from `cwl-eval`\n",
    "\n",
    "In `compatiability/evals/*`, we have the output from `cwl-eval` for several retrieval models: BM25, a Language Model (LM) and Divergence from Randomness Model (PL2) on the TREC AP collection.\n",
    "\n",
    "- ap_bm25.eval\t\n",
    "- ap_lmd.eval\t\n",
    "- ap_pl2.eval\n",
    "\n",
    "The result files for each retrieval model and the qrels file used are also in the `compatiability` folder.\n",
    "\n",
    "    cwl-eval qrels/trec_ap_51-200.qrels results/ap_bm25.res -n > evals/ap_bm25.eval\n",
    "    cwl-eval qrels/trec_ap_51-200.qrels results/ap_lmd.res -n > evals/ap_lmd.eval \n",
    "    cwl-eval qrels/trec_ap_51-200.qrels results/ap_pl2.res -n > evals/ap_pl2.eval \n",
    "    \n",
    "Note that have included the `-n` argument - which includes the column headings. \n",
    "\n",
    "Now lets load these eval files into dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the `cwl-eval` output into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfbm25 = pd.read_csv('../compatibility/evals/ap_bm25.eval',sep='\\t')\n",
    "dflmd = pd.read_csv('../compatibility/evals/ap_lmd.eval',sep='\\t')\n",
    "dfpl2 = pd.read_csv('../compatibility/evals/ap_pl2.eval',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing the Columns in `cwl-eval` output\n",
    "\n",
    "By default `cwl-eval` will output six columns (see below), where EU = Expected Utility per Item, ETU = Expected Total Utility, EC = Expected Cost per Item, ETC = Expected Total Cost and ED is Expected Depth (or Expected Number of Items Examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic\n",
      "Metric\n",
      "EU\n",
      "ETU\n",
      "EC\n",
      "ETC\n",
      "ED\n"
     ]
    }
   ],
   "source": [
    "fields = dfbm25.columns\n",
    "for f in fields:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing the Metrics in `cwl-eval` output\n",
    "\n",
    "By default `cwl-eval` will output a subset of metrics (but you can specific the metrics you want with the `-m` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1\n",
      "P@2\n",
      "P@3\n",
      "P@4\n",
      "P@5\n",
      "P@10\n",
      "RBP@0.2\n",
      "RBP@0.4\n",
      "RBP@0.8\n",
      "NDCG-k@5\n",
      "NDCG-k@10\n",
      "RR\n",
      "AP\n",
      "INST-T=1.0\n",
      "INST-T=2.0\n",
      "INST-T=3.0\n"
     ]
    }
   ],
   "source": [
    "metrics = dfbm25['Metric'].unique()\n",
    "for m in metrics:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting the Mean and Standard Error\n",
    "\n",
    "Below we have an example where we group by the metric, and report the mean and standard error of: EU, ETU and ED for BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">EU</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ETU</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ED</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>0.315</td>\n",
       "      <td>0.019</td>\n",
       "      <td>10.182</td>\n",
       "      <td>0.889</td>\n",
       "      <td>61.243</td>\n",
       "      <td>7.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INST-T=1.0</th>\n",
       "      <td>0.498</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.043</td>\n",
       "      <td>1.878</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INST-T=2.0</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.225</td>\n",
       "      <td>0.066</td>\n",
       "      <td>3.326</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INST-T=3.0</th>\n",
       "      <td>0.426</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.701</td>\n",
       "      <td>0.090</td>\n",
       "      <td>4.824</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG-k@10</th>\n",
       "      <td>0.419</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.903</td>\n",
       "      <td>0.122</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG-k@5</th>\n",
       "      <td>0.439</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.294</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2.949</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@1</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.041</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@10</th>\n",
       "      <td>0.403</td>\n",
       "      <td>0.026</td>\n",
       "      <td>4.027</td>\n",
       "      <td>0.264</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@2</th>\n",
       "      <td>0.477</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.069</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@3</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.313</td>\n",
       "      <td>0.093</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@4</th>\n",
       "      <td>0.433</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.733</td>\n",
       "      <td>0.118</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@5</th>\n",
       "      <td>0.425</td>\n",
       "      <td>0.029</td>\n",
       "      <td>2.127</td>\n",
       "      <td>0.143</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBP@0.2</th>\n",
       "      <td>0.479</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBP@0.4</th>\n",
       "      <td>0.465</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.055</td>\n",
       "      <td>1.667</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBP@0.8</th>\n",
       "      <td>0.417</td>\n",
       "      <td>0.027</td>\n",
       "      <td>2.083</td>\n",
       "      <td>0.133</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RR</th>\n",
       "      <td>0.608</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "      <td>24.440</td>\n",
       "      <td>8.805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               EU            ETU             ED       \n",
       "             mean    sem    mean    sem    mean    sem\n",
       "Metric                                                \n",
       "AP          0.315  0.019  10.182  0.889  61.243  7.925\n",
       "INST-T=1.0  0.498  0.033   0.741  0.043   1.878  0.040\n",
       "INST-T=2.0  0.450  0.029   1.225  0.066   3.326  0.065\n",
       "INST-T=3.0  0.426  0.027   1.701  0.090   4.824  0.088\n",
       "NDCG-k@10   0.419  0.027   1.903  0.122   4.544  0.000\n",
       "NDCG-k@5    0.439  0.029   1.294  0.086   2.949  0.000\n",
       "P@1         0.487  0.041   0.487  0.041   1.000  0.000\n",
       "P@10        0.403  0.026   4.027  0.264  10.000  0.000\n",
       "P@2         0.477  0.035   0.953  0.069   2.000  0.000\n",
       "P@3         0.438  0.031   1.313  0.093   3.000  0.000\n",
       "P@4         0.433  0.030   1.733  0.118   4.000  0.000\n",
       "P@5         0.425  0.029   2.127  0.143   5.000  0.000\n",
       "RBP@0.2     0.479  0.037   0.599  0.046   1.250  0.000\n",
       "RBP@0.4     0.465  0.033   0.775  0.055   1.667  0.000\n",
       "RBP@0.8     0.417  0.027   2.083  0.133   5.000  0.000\n",
       "RR          0.608  0.033   0.993  0.007  24.440  8.805"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbm25.groupby('Metric')['EU','ETU','ED'].agg(['mean','sem']).round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Runs\n",
    "\n",
    "\n",
    "First we need to give each result list a name - this is done by using the `insert` command on the dataframe.\n",
    "\n",
    "Then, we need to concatenate all the results together.\n",
    "\n",
    "To perform the statistica testing we will be using the Pingouin Python Package.\n",
    "\n",
    "We shall focus our attention on testing whether the Expected Utility (EU) is similar for Precision at 10 (P@10).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bm25</th>\n",
       "      <th>P@10</th>\n",
       "      <td>0.403</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lmd</th>\n",
       "      <th>P@10</th>\n",
       "      <td>0.423</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl2</th>\n",
       "      <th>P@10</th>\n",
       "      <td>0.432</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean    sem\n",
       "Name Metric              \n",
       "bm25 P@10    0.403  0.026\n",
       "lmd  P@10    0.423  0.028\n",
       "pl2  P@10    0.432  0.027"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'Name' not in dfbm25.columns:\n",
    "    dfbm25.insert(0,'Name','bm25')\n",
    "    dflmd.insert(0,'Name','lmd')\n",
    "    dfpl2.insert(0,'Name','pl2')\n",
    "\n",
    "dfall = pd.concat([dfbm25, dflmd, dfpl2])\n",
    "\n",
    "import pingouin as pg\n",
    "\n",
    "metric = 'P@10'\n",
    "measurement = 'EU'\n",
    "\n",
    "# select the metric we are interested in doing the comparison over\n",
    "dftest = dfall.loc[dfall['Metric'] == metric]\n",
    "\n",
    "# show a table of the different runs for the expected utility\n",
    "dftest.groupby(['Name','Metric'])['EU'].agg(['mean','sem']).round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is this Significantly Different?\n",
    "\n",
    "Which one is better? Our table shows that PL2 has the highest P@10 = 0.432. But is this really better than BM25 and LMD?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a Repeated Measures ANOVA\n",
    "\n",
    "To find out if there is potentially a significant difference between the runs, we first perform a Repeated Measures ANOVA - this is because each topic provides a different measurement, and each run provides a measurement for each topic. So our within variable is the `Name` of our run, and the the `Topic` variable each subject.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Source     SS   DF     MS     F       p-unc  p-GG-corr    np2    eps  \\\n",
      "0   Name  0.068    2  0.034  4.98  0.00745722  0.0122786  0.032  0.807   \n",
      "1  Error  2.039  298  0.007     -           -          -      -      -   \n",
      "\n",
      "  sphericity W-spher      p-spher  \n",
      "0      False   0.761  1.62009e-09  \n",
      "1          -       -            -  \n"
     ]
    }
   ],
   "source": [
    "aov = pg.rm_anova(data=dftest, dv=measurement , within=['Name'], subject='Topic', detailed=True)\n",
    "print(aov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform follow up significance testing using Pairwise T-Tests with Bonferroni Correction\n",
    "Since the ANOVA came back with showing that the corrected p-value `p-GG-corr` is less that 0.05, it motivates performing a follow up test to find out which run pairs are different. It is tempting to use `p-unc`, but this is on valid if you have two runs that are being compared. Note that `np2` is the partial effect size, where <0.06 is a small effect size, 0.006-0.14 is a medium effect size, and >0.14 is a large effect size. (see J. Cohen. 1973.   Eta-squared and partial eta-squared in fixed factor ANOVAdesigns.Educational and psychological measurement33, 1 (1973), 107â€“112.)\n",
    "\n",
    "\n",
    "So to find out which pairs are different, we need to perform a pairwise T-Test with Bonfferroni Correction.\n",
    "Note: when interpreting the follow up T-Tests we need to use the corrected p-values, i.e. `p-corr` not the uncorrected p-values i.e. `p-unc`\n",
    "\n",
    "If you are only comparing two systems/runs, then no correction is needed, and so only `p-unc` is reported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Contrast     A    B  Paired  Parametric      T    dof       Tail     p-unc  \\\n",
      "0     Name  bm25  lmd    True        True -1.812  149.0  two-sided  0.071930   \n",
      "1     Name  bm25  pl2    True        True -3.077  149.0  two-sided  0.002488   \n",
      "2     Name   lmd  pl2    True        True -1.193  149.0  two-sided  0.234613   \n",
      "\n",
      "     p-corr p-adjust   BF10  hedges  \n",
      "0  0.215789     bonf  0.448  -0.062  \n",
      "1  0.007463     bonf  8.252  -0.089  \n",
      "2  0.703838     bonf  0.182  -0.026  \n"
     ]
    }
   ],
   "source": [
    "pt = pg.pairwise_ttests(dv=measurement, within=['Name'], subject='Topic', data=dftest, padjust='bonf')\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who wins?\n",
    "\n",
    "From the pairwise comparison, we can see that only BM25 vs PL2 shows a significant difference with corrected p-value of p=0.007463.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
